# Awesome Large Foundation Model Theory

Welcome to the Awesome Large Foundation Model Theory repository! This repository is dedicated to exploring and discussing the fascinating field of In Context Learning Theory.

## About

Large Foundation Modal Theory examines the ways in which learning is enhanced when it occurs within meaningful and relevant contexts. This repository aims to gather resources, research papers, presentations, and foster discussions related to Large Foundation Modal Theory, its applications, and its impact on education and learning environments. In this group, we study the Large Foundation Modal Theory, which includes but is not limited to


1. [In context learning](#In_context_learning)
2. [Diffusion Model](#Diffusion_Model)
3. [Hallucination](#Hallucination)
4. [Chain-of-Thought](#Chain-of-Thought)
 

## In Context Learning

- What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. *NeurIPS 2022*, [(link)](https://arxiv.org/abs/2208.01066)

  Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant

- What learning algorithm is in-context learning? Investigations with linear models. *ICLR 2023*. [(link)](https://arxiv.org/pdf/2211.15661.pdf)

  Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou

- Trained Transformers Learn Linear Models In-Context. [(link)](https://arxiv.org/pdf/2306.09927.pdf)

  Ruiqi Zhang, Spencer Frei, Peter L. Bartlett

- Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning, *NeurIPS 2023* [(link)](https://arxiv.org/abs/2301.11916)

  Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

- In-Context Learning through the Bayesian Prism. [(link)](https://arxiv.org/abs/2306.04891)

  Kabir Ahuja, Madhur Panwar, Navin Goyal

- What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization, [(link)](https://arxiv.org/abs/2305.19420)

  Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, Zhaoran Wang
  

  
## Diffusion Model

- Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, *ICML 2023*, [(link)](https://arxiv.org/pdf/2304.12824.pdf)

  Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, Jun Zhu

- Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization, [(link)](https://openreview.net/pdf?id=h8GeqOxtd4)

- Learning Mixtures of Gaussians Using the DDPM Objectiveï¼Œ*NeurIPS 2023*,  [(link)](https://arxiv.org/pdf/2307.01178.pdf)

  Kulin Shah, Sitan Chen, Adam Klivans

- Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo, [(link)](https://arxiv.org/abs/2401.06325)

  Xunpeng Huang, Difan Zou, Hanze Dong, Yian Ma, Tong Zhang


## Hallucination

- Calibrated Language Models Must Hallucinate, [(link)](https://arxiv.org/abs/2311.14648)

  Adam Tauman Kalai, Santosh S. Vempala


## Chain-of-Thought

- Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. *NeurIPS 2023*, [(link)](https://arxiv.org/abs/2305.18869)

  Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, Samet Oymak


## How to Contribute

We welcome contributions from anyone interested in In Context Learning Theory. Here are some ways you can contribute:

- **Add resources:** Share relevant research papers, articles, books, or other resources by opening a pull request.
- **Start a discussion:** Create a new discussion thread in the GitHub Discussions tab to initiate conversations and share insights.
- **Suggest improvements:** If you have any suggestions or ideas to improve the repository, open an issue and let us know.
- **Spread the word:** Help us reach more people by sharing this repository with others who might be interested.

Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file for more detailed instructions on how to contribute.

## Code of Conduct

To ensure that this repository remains a welcoming and inclusive space for everyone, we have adopted a [Code of Conduct](CODE_OF_CONDUCT.md). We kindly request all contributors to adhere to these guidelines when participating in this community.

## License

This repository is licensed under the [MIT License](LICENSE). Please note that any contributions made to this repository will be subject to the same license.
