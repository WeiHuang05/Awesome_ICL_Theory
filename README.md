# Awesome Large Foundation Model Theory

Welcome to the Awesome Large Foundation Model Theory repository! This repository is dedicated to exploring and discussing the fascinating field of In Context Learning Theory.

## About

Large Foundation Modal Theory examines the ways in which learning is enhanced when it occurs within meaningful and relevant contexts. This repository aims to gather resources, research papers, presentations, and foster discussions related to Large Foundation Modal Theory, its applications, and its impact on education and learning environments. In this group, we study the Large Foundation Modal Theory, which includes but is not limited to


1. [In context learning](#In_context_learning)
2. [Diffusion Model](#Diffusion_Model)
3. [Hallucination](#Hallucination)
4. [Chain-of-Thought](#Chain-of-Thought)
 

## In Context Learning

### 2022

> **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes**, *NeurIPS 2022*, [link](https://arxiv.org/abs/2208.01066)  
> Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant 

---

> **Data Distributional Properties Drive Emergent In-Context Learning in Transformers**, *NeurIPS 2022*, [link](https://arxiv.org/abs/2205.05055)  
> Stephanie C.Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, Felix Hill  

---

> **In-context learning and induction heads**  
> *Transformer Circuits Thread, 2022*  
> Catherine Olsson, Nelson Elhage, Neel Nanda, et al.  
> [link](https://arxiv.org/abs/2209.11895)

---

> **An Explanation of In-context Learning as Implicit Bayesian Inference**  
> *ICLR 2022*  
> Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma  
> [link](https://arxiv.org/abs/2111.02080)

### 2023

---

> **What learning algorithm is in-context learning? Investigations with linear models**  
> *ICLR 2023*  
> Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou  
> [link](https://arxiv.org/pdf/2211.15661.pdf)

---

> Uncovering mesa-optimization algorithms in Transformers, [(link)](https://arxiv.org/abs/2309.05858)
> Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise Agüera y Arcas, Max Vladymyrov, Razvan Pascanu, João Sacramento


---

> **Transformers as statisticians: Provable in-context learning with in-context algorithm selection**  
> *NeurIPS 2023*  
> Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei  
> [link](https://arxiv.org/abs/2306.04637)

---

> **Transformers learn in-context by gradient descent**  
> *ICML 2023*  
> Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov  
> [link](https://arxiv.org/abs/2212.07677)

---

> **Max-Margin Token Selection in Attention Mechanism**  
> *NeurIPS 2023*  
> Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak  
> [link](https://arxiv.org/abs/2306.13596)

---

> **Transformers learn to implement preconditioned gradient descent for in-context learning**  
> *NeurIPS 2023*  
> Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, Suvrit Sra  
> [link](https://arxiv.org/abs/2306.00297)

---

> **Trained Transformers Learn Linear Models In-Context**  
> Ruiqi Zhang, Spencer Frei, Peter L. Bartlett  
> [link](https://arxiv.org/pdf/2306.09927.pdf)

---

> **In-context convergence of transformers**  
> Yu Huang, Yuan Cheng, Yingbin Liang  
> [link](https://arxiv.org/abs/2310.05249)

---

> **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning**  
> *NeurIPS 2023*  
> Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang  
> [link](https://arxiv.org/abs/2301.11916)

---

> **In-Context Learning through the Bayesian Prism**  
> Kabir Ahuja, Madhur Panwar, Navin Goyal  
> [link](https://arxiv.org/abs/2306.04891)

---

> **What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization**  
> Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, Zhaoran Wang  
> [link](https://arxiv.org/abs/2305.19420)

---

> **Birth of a Transformer: A Memory Viewpoint**  
> *NeurIPS 2023*  
> Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, Leon Bottou  
> [link](https://arxiv.org/abs/2306.00802)

### 2024

---

> **How Transformers Learn Causal Structure with Gradient Descent**  
> *ICML 2024*  
> Eshaan Nichani, Alex Damian, Jason D. Lee  
> [link](https://arxiv.org/abs/2402.14735)


 
  
## Diffusion_Model

- Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, *ICML 2023*, [(link)](https://arxiv.org/pdf/2304.12824.pdf)

  Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, Jun Zhu

- Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization, [(link)](https://openreview.net/pdf?id=h8GeqOxtd4)

- Learning Mixtures of Gaussians Using the DDPM Objective，*NeurIPS 2023*,  [(link)](https://arxiv.org/pdf/2307.01178.pdf)

  Kulin Shah, Sitan Chen, Adam Klivans

- Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo, [(link)](https://arxiv.org/abs/2401.06325)

  Xunpeng Huang, Difan Zou, Hanze Dong, Yian Ma, Tong Zhang

- Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization, *ICLR 2024*, [(link)](https://arxiv.org/abs/2401.15604)

  Yinbin Han, Meisam Razaviyayn, Renyuan Xu


## Hallucination

- Calibrated Language Models Must Hallucinate, [(link)](https://arxiv.org/abs/2311.14648)

  Adam Tauman Kalai, Santosh S. Vempala


## Chain-of-Thought

- Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. *NeurIPS 2023*, [(link)](https://arxiv.org/abs/2305.18869)

  Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, Samet Oymak


## How to Contribute

We welcome contributions from anyone interested in In Context Learning Theory. Here are some ways you can contribute:

- **Add resources:** Share relevant research papers, articles, books, or other resources by opening a pull request.
- **Start a discussion:** Create a new discussion thread in the GitHub Discussions tab to initiate conversations and share insights.
- **Suggest improvements:** If you have any suggestions or ideas to improve the repository, open an issue and let us know.
- **Spread the word:** Help us reach more people by sharing this repository with others who might be interested.

Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file for more detailed instructions on how to contribute.

## Code of Conduct

To ensure that this repository remains a welcoming and inclusive space for everyone, we have adopted a [Code of Conduct](CODE_OF_CONDUCT.md). We kindly request all contributors to adhere to these guidelines when participating in this community.

## License

This repository is licensed under the [MIT License](LICENSE). Please note that any contributions made to this repository will be subject to the same license.
