# Awesome In Large Foundation Model Theory

Welcome to the Awesome In Context Learning Theory repository! This repository is dedicated to exploring and discussing the fascinating field of In Context Learning Theory.

## About

In Context Learning Theory examines the ways in which learning is enhanced when it occurs within meaningful and relevant contexts. This repository aims to gather resources, research papers, presentations, and foster discussions related to In Context Learning Theory, its applications, and its impact on education and learning environments.

## Goals

- Create a collaborative space for scholars, researchers, educators, and students interested in In Context Learning Theory.
- Share and curate a collection of high-quality resources, including research papers, books, articles, and other relevant materials.
- Foster discussions and exchange ideas on various aspects of In Context Learning Theory.
- Explore the practical applications and implications of In Context Learning Theory in educational settings.
- Provide a supportive environment for individuals to learn, contribute, and collaborate in the field of In Context Learning Theory.

## In Context Learning

- What learning algorithm is in-context learning? Investigations with linear models. *ICLR 2023*. [(link)](https://arxiv.org/pdf/2211.15661.pdf)

  Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou

- Trained Transformers Learn Linear Models In-Context. [(link)](https://arxiv.org/pdf/2306.09927.pdf)

  Ruiqi Zhang, Spencer Frei, Peter L. Bartlett

- Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning, *NeurIPS 2023* [(link)](https://arxiv.org/abs/2301.11916)

  Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang

- In-Context Learning through the Bayesian Prism. [(link)](https://arxiv.org/abs/2306.04891)

  Kabir Ahuja, Madhur Panwar, Navin Goyal

- What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization, [(link)](https://arxiv.org/abs/2305.19420)

  Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, Zhaoran Wang
  

  
## Diffusion Model

- Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, *ICML 2023*, [(link)](https://arxiv.org/pdf/2304.12824.pdf)

  Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, Jun Zhu

- Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization, [(link)](https://openreview.net/pdf?id=h8GeqOxtd4)

- Learning Mixtures of Gaussians Using the DDPM Objectiveï¼Œ*NeurIPS 2023*,  [(link)](https://arxiv.org/pdf/2307.01178.pdf)

  Kulin Shah, Sitan Chen, Adam Klivans

- Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo, [(link)](https://arxiv.org/abs/2401.06325)

  Xunpeng Huang, Difan Zou, Hanze Dong, Yian Ma, Tong Zhang



## How to Contribute

We welcome contributions from anyone interested in In Context Learning Theory. Here are some ways you can contribute:

- **Add resources:** Share relevant research papers, articles, books, or other resources by opening a pull request.
- **Start a discussion:** Create a new discussion thread in the GitHub Discussions tab to initiate conversations and share insights.
- **Suggest improvements:** If you have any suggestions or ideas to improve the repository, open an issue and let us know.
- **Spread the word:** Help us reach more people by sharing this repository with others who might be interested.

Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file for more detailed instructions on how to contribute.

## Code of Conduct

To ensure that this repository remains a welcoming and inclusive space for everyone, we have adopted a [Code of Conduct](CODE_OF_CONDUCT.md). We kindly request all contributors to adhere to these guidelines when participating in this community.

## License

This repository is licensed under the [MIT License](LICENSE). Please note that any contributions made to this repository will be subject to the same license.

---

We look forward to your contributions and engaging discussions as we explore the exciting field of In Context Learning Theory!
