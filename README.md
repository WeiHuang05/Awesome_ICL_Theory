# Awesome Large Foundation Model Theory

Welcome to the Awesome Large Foundation Model Theory repository! This repository is dedicated to exploring and discussing the fascinating field of Large Foundation Model Theory.

## About

Large Foundation Modal Theory examines the ways in which learning is enhanced when it occurs within meaningful and relevant contexts. This repository aims to gather resources, research papers, presentations, and foster discussions related to Large Foundation Modal Theory, its applications, and its impact on education and learning environments. In this group, we study the Large Foundation Modal Theory, which includes but is not limited to


1. [In context learning](#In_context_learning)
2. [Diffusion Model](#diffusion-model)
3. [Hallucination](#Hallucination)
4. [Chain-of-Thought](#Chain-of-Thought)
5. [Reasoning](#Reasoning)
6. [State Space Models](#State_space_models)
 

## In Context Learning

> What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, *NeurIPS 2022*, [link](https://arxiv.org/abs/2208.01066)  

> Data Distributional Properties Drive Emergent In-Context Learning in Transformers, *NeurIPS 2022*, [link](https://arxiv.org/abs/2205.05055)  

> In-context learning and induction heads**, *Transformer Circuits Thread, 2022*, [link](https://arxiv.org/abs/2209.11895)

> An Explanation of In-context Learning as Implicit Bayesian Inference, *ICLR 2022*, [link](https://arxiv.org/abs/2111.02080)

### 2023

> What learning algorithm is in-context learning? Investigations with linear models, *ICLR 2023*, [link](https://arxiv.org/pdf/2211.15661.pdf)  

> Uncovering mesa-optimization algorithms in Transformers, [link](https://arxiv.org/abs/2309.05858)  

> Transformers as statisticians: Provable in-context learning with in-context algorithm selection, *NeurIPS 2023*,  [link](https://arxiv.org/abs/2306.04637)

> Transformers as Algorithms: Generalization and Stability in In-context Learning, *ICML 2023*,  [link](https://proceedings.mlr.press/v202/li23l/li23l.pdf)

> Transformers learn in-context by gradient descent, *ICML 2023*, [link](https://arxiv.org/abs/2212.07677)

> Max-Margin Token Selection in Attention Mechanism, *NeurIPS 2023*, [link](https://arxiv.org/abs/2306.13596)  

> Transformers learn to implement preconditioned gradient descent for in-context learning, *NeurIPS 2023*, [link](https://arxiv.org/abs/2306.00297)

> Trained Transformers Learn Linear Models In-Context, *JMLR*, [link](https://arxiv.org/pdf/2306.09927.pdf)

> In-context convergence of transformers, *ICML 2024*, [link](https://arxiv.org/abs/2310.05249)

> Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning, *NeurIPS 2023*, [link](https://arxiv.org/abs/2301.11916)

> In-Context Learning through the Bayesian Prism, *ICLR 2024* [link](https://arxiv.org/abs/2306.04891)

> What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization**, [link](https://arxiv.org/abs/2305.19420)

> Birth of a Transformer: A Memory Viewpoint, *NeurIPS 2023*, [link](https://arxiv.org/abs/2306.00802)

> A Theory of Emergent In-Context Learning as Implicit Structure Induction, [link](https://arxiv.org/pdf/2303.07971)

### 2024

> How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression, *ICLR 2024* [link](https://arxiv.org/abs/2310.08391)

> How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?, *ICML 2024*, [link](https://openreview.net/forum?id=I4HTPws9P6)  

> How Transformers Learn Causal Structure with Gradient Descent, *ICML 2024*, [link](https://arxiv.org/abs/2402.14735)  

> How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes, [link](https://arxiv.org/pdf/2404.03558)  

> Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?, *ICML 2024*, [link](https://openreview.net/pdf?id=o8AaRKbP9K)

> Transformers learn nonlinear features in context: nonconvex mean-field dynamics on the attention landscape, *ICML 2024*, [link](https://arxiv.org/abs/2402.01258)

> How Well Can Transformers Emulate In-context Newton's Method?, [link](https://arxiv.org/pdf/2403.03183)  

> In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization, [link](https://arxiv.org/pdf/2402.14951)  

> The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains, [link](https://arxiv.org/pdf/2402.11004)  

> Fine-grained Analysis of In-context Linear Estimation, [link](https://openreview.net/pdf?id=1vM1a7KrC6)

> On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability, NeurIPS 2024, [link](https://arxiv.org/pdf/2405.16845)

> Transformers are Minimax Optimal Nonparametric In-Context Learners, NeurIPS 2024, [link](https://arxiv.org/pdf/2408.12186)

> Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context, [link](https://arxiv.org/abs/2410.01774)






## Diffusion Model

> Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, *ICML 2023*, [(link)](https://arxiv.org/pdf/2304.12824.pdf)

> Diffusion Models are Minimax Optimal Distribution Estimators, *ICML 2023*, [(link)](https://arxiv.org/pdf/2303.01861)

> Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data, *ICML 2023*, [(link)](https://arxiv.org/pdf/2302.07194)

> The probability flow ODE is provably fast, *NeurIPS 2023*,  [(link)](https://proceedings.neurips.cc/paper_files/paper/2023/hash/d84a27ff694345aacc21c72097a69ea2-Abstract-Conference.html)

> Learning Mixtures of Gaussians Using the DDPM Objectiveï¼Œ*NeurIPS 2023*,  [(link)](https://arxiv.org/pdf/2307.01178.pdf)

> On the Generalization Properties of Diffusion Models, *NeurIPS 2023*, [link](https://arxiv.org/pdf/2311.01797)

> Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models, *M3L (2023)*, [(link)](https://arxiv.org/pdf/2309.11420)

> Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo, *COLT 2024*, [(link)](https://arxiv.org/abs/2401.06325)

> Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization, *ICLR 2024*, [(link)](https://arxiv.org/abs/2401.15604)

> Critical windows: non-asymptotic theory for feature emergence in diffusion models, *ICML 2024*, [(link)](https://arxiv.org/pdf/2403.01633)

> Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions, *ICML 2024*,  [(link)](https://arxiv.org/abs/2402.15602)

> Reverse Transition Kernel: A Flexible Framework to Accelerate Diffusion Inference, *ICML 2024*,  [(link)](https://arxiv.org/abs/2405.16387)

> Learning General Gaussian Mixtures with Efficient Score Matching, Apr 2024, [(link)](https://arxiv.org/abs/2404.18893)

> Learning Mixtures of Gaussians Using Diffusion Models, Apr 2024, [(link)](https://arxiv.org/pdf/2404.18869)

> An overview of diffusion models: Applications, guided generation, statistical rates and optimization, Apr 2024, [(link)](https://arxiv.org/abs/2404.07771)

> Slight Corruption in Pre-training Data Makes Better Diffusion Models, May 2024, [(link)](https://arxiv.org/abs/2405.20494)

> Accelerating Convergence of Score-Based Diffusion Models, Provably, May 2024, [(link)](https://arxiv.org/abs/2403.03852)

> Unraveling the Smoothness Properties of Diffusion Models: A Gaussian Mixture Perspective, May 2024, [(link)](https://arxiv.org/pdf/2405.16418)

> U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models, May 2024, [(link)](https://arxiv.org/pdf/2404.18444)

> Extracting Training Data from Unconditional Diffusion Models, June 2024, [(link)](https://arxiv.org/abs/2406.12752)

> On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs), July, 2024, [(link)](https://arxiv.org/pdf/2407.01079)

> Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering, Sep, 2024, [(link)](https://arxiv.org/pdf/2407.01079)


## Chain-of-Thought

> Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. *NeurIPS 2023*, [(link)](https://arxiv.org/abs/2305.18869)

> Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective. *NeurIPS 2023*, [(link)](https://www.arxiv.org/abs/2409.02426)

> Chain of Thought Empowers Transformers to Solve Inherently Serial Problems. *ICLR 2024*, [(link)](https://arxiv.org/abs/2402.12875)



## Hallucination

> Calibrated Language Models Must Hallucinate, [(link)](https://arxiv.org/abs/2311.14648)

## Reasoning

> How Much Can RAG Help the Reasoning of LLM? [(link)](https://arxiv.org/abs/2410.02338)


## State Space Models

> Repeat After Me: Transformers are Better than State Space Models at Copying, *ICML 2024*, [(link)](https://openreview.net/pdf?id=duRRoGeoQT)


## How to Contribute

We welcome contributions from anyone interested in Large Foundation Model Theory. Here are some ways you can contribute:

- **Add resources:** Share relevant research papers, articles, books, or other resources by opening a pull request.
- **Start a discussion:** Create a new discussion thread in the GitHub Discussions tab to initiate conversations and share insights.
- **Suggest improvements:** If you have any suggestions or ideas to improve the repository, open an issue and let us know.
- **Spread the word:** Help us reach more people by sharing this repository with others who might be interested.

Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file for more detailed instructions on how to contribute.

## Code of Conduct

To ensure that this repository remains a welcoming and inclusive space for everyone, we have adopted a [Code of Conduct](CODE_OF_CONDUCT.md). We kindly request all contributors to adhere to these guidelines when participating in this community.

## License

This repository is licensed under the [MIT License](LICENSE). Please note that any contributions made to this repository will be subject to the same license.
