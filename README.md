# Awesome Large Foundation Model Theory

Welcome to the Awesome Large Foundation Model Theory repository! This repository is dedicated to exploring and discussing the fascinating field of In Context Learning Theory.

## About

Large Foundation Modal Theory examines the ways in which learning is enhanced when it occurs within meaningful and relevant contexts. This repository aims to gather resources, research papers, presentations, and foster discussions related to Large Foundation Modal Theory, its applications, and its impact on education and learning environments. In this group, we study the Large Foundation Modal Theory, which includes but is not limited to


1. [In context learning](#In_context_learning)
2. [Diffusion Model](#Diffusion_Model)
3. [Hallucination](#Hallucination)
4. [Chain-of-Thought](#Chain-of-Thought)
 

## In Context Learning

### 2022

> **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes**, *NeurIPS 2022*, [link](https://arxiv.org/abs/2208.01066)  
---

> **Data Distributional Properties Drive Emergent In-Context Learning in Transformers**, *NeurIPS 2022*, [link](https://arxiv.org/abs/2205.05055)  
---

> **In-context learning and induction heads**, *Transformer Circuits Thread, 2022*, [link](https://arxiv.org/abs/2209.11895)
---

> **An Explanation of In-context Learning as Implicit Bayesian Inference**, *ICLR 2022*, [link](https://arxiv.org/abs/2111.02080)

### 2023


> **What learning algorithm is in-context learning? Investigations with linear models**, *ICLR 2023*, [link](https://arxiv.org/pdf/2211.15661.pdf)  
---

> **Uncovering mesa-optimization algorithms in Transformers**, [link](https://arxiv.org/abs/2309.05858)  
---

> **Transformers as statisticians: Provable in-context learning with in-context algorithm selection**, *NeurIPS 2023*,  [link](https://arxiv.org/abs/2306.04637)
---

> **Transformers as Algorithms: Generalization and Stability in In-context Learning**, *ICML 2023*,  [link](https://proceedings.mlr.press/v202/li23l/li23l.pdf)
---

> **Transformers learn in-context by gradient descent**, *ICML 2023*, [link](https://arxiv.org/abs/2212.07677)
---

> **Max-Margin Token Selection in Attention Mechanism**, *NeurIPS 2023*, [link](https://arxiv.org/abs/2306.13596)  
---

> **Transformers learn to implement preconditioned gradient descent for in-context learning**, *NeurIPS 2023*, [link](https://arxiv.org/abs/2306.00297)
---

> **Trained Transformers Learn Linear Models In-Context**, *JMLR*, [link](https://arxiv.org/pdf/2306.09927.pdf)
---

> **In-context convergence of transformers**, *ICML 2024*, [link](https://arxiv.org/abs/2310.05249)
---

> **Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning**, *NeurIPS 2023*, [link](https://arxiv.org/abs/2301.11916)
---

> **In-Context Learning through the Bayesian Prism**, [link](https://arxiv.org/abs/2306.04891)
---

> **What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization**, [link](https://arxiv.org/abs/2305.19420)
---

> **Birth of a Transformer: A Memory Viewpoint**, *NeurIPS 2023*, [link](https://arxiv.org/abs/2306.00802)
---

> **A Theory of Emergent In-Context Learning as Implicit Structure Induction**, [link](https://arxiv.org/pdf/2303.07971)

### 2024

> **How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?**, *ICML 2024*, [link](https://openreview.net/forum?id=I4HTPws9P6)  
---

> **How Transformers Learn Causal Structure with Gradient Descent**, *ICML 2024*, [link](https://arxiv.org/abs/2402.14735)  
---

> **How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes**, [link](https://arxiv.org/pdf/2404.03558)  
---

> **Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?**, *ICML 2024*, [link](https://openreview.net/pdf?id=o8AaRKbP9K)
 ---

> **How Well Can Transformers Emulate In-context Newton's Method?**, [link](https://arxiv.org/pdf/2403.03183)  
 ---

> **In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization**, [link](https://arxiv.org/pdf/2402.14951)  
 ---

> **The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains**, [link](https://arxiv.org/pdf/2402.11004)  
 ---

> **Fine-grained Analysis of In-context Linear Estimation**, [link](https://openreview.net/pdf?id=1vM1a7KrC6)  

## Diffusion Model

- Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning, *ICML 2023*, [(link)](https://arxiv.org/pdf/2304.12824.pdf)

- Diffusion Models are Minimax Optimal Distribution Estimators, *ICML 2023*, [(link)](https://arxiv.org/pdf/2303.01861)

- Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data, *ICML 2023*, [(link)](https://arxiv.org/pdf/2302.07194)

- Learning Mixtures of Gaussians Using the DDPM Objectiveï¼Œ*NeurIPS 2023*,  [(link)](https://arxiv.org/pdf/2307.01178.pdf)

- On the Generalization Properties of Diffusion Models, *NeurIPS 2023*, [link](https://arxiv.org/pdf/2311.01797)

- Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo, [(link)](https://arxiv.org/abs/2401.06325)

- Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization, *ICLR 2024*, [(link)](https://arxiv.org/abs/2401.15604)

- Critical windows: non-asymptotic theory for feature emergence in diffusion models, *ICML 2024*, [(link)](https://arxiv.org/pdf/2403.01633)

- Learning General Gaussian Mixtures with Efficient Score Matching, Apr 2024, [(link)](https://arxiv.org/abs/2404.18893)

- Learning Mixtures of Gaussians Using Diffusion Models, Apr 2024, [(link)](https://arxiv.org/pdf/2404.18869)

- Unraveling the Smoothness Properties of Diffusion Models: A Gaussian Mixture Perspective, May 2024, [(link)](https://arxiv.org/pdf/2405.16418)

- On Statistical Rates and Provably Efficient Criteria of Latent Diffusion Transformers (DiTs), July, 2024, [(link)](https://arxiv.org/pdf/2407.01079)




## Hallucination

- Calibrated Language Models Must Hallucinate, [(link)](https://arxiv.org/abs/2311.14648)

  Adam Tauman Kalai, Santosh S. Vempala


## Chain-of-Thought

- Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning. *NeurIPS 2023*, [(link)](https://arxiv.org/abs/2305.18869)

  Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, Samet Oymak


## How to Contribute

We welcome contributions from anyone interested in In Context Learning Theory. Here are some ways you can contribute:

- **Add resources:** Share relevant research papers, articles, books, or other resources by opening a pull request.
- **Start a discussion:** Create a new discussion thread in the GitHub Discussions tab to initiate conversations and share insights.
- **Suggest improvements:** If you have any suggestions or ideas to improve the repository, open an issue and let us know.
- **Spread the word:** Help us reach more people by sharing this repository with others who might be interested.

Please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file for more detailed instructions on how to contribute.

## Code of Conduct

To ensure that this repository remains a welcoming and inclusive space for everyone, we have adopted a [Code of Conduct](CODE_OF_CONDUCT.md). We kindly request all contributors to adhere to these guidelines when participating in this community.

## License

This repository is licensed under the [MIT License](LICENSE). Please note that any contributions made to this repository will be subject to the same license.
